{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find-s Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "    return data\n",
    "\n",
    "def find_s(data):\n",
    "    hypothesis = data[0][:-1]\n",
    "    for instance in data:\n",
    "        if instance[-1] == 'yes': \n",
    "            for i in range(len(hypothesis)):\n",
    "                if instance[i] != hypothesis[i]:\n",
    "                    hypothesis[i] = '?'\n",
    "                    \n",
    "    return hypothesis\n",
    "\n",
    "# Example usage:\n",
    "data = read_csv(r'E:\\ML LAB\\1ds.csv')\n",
    "hypothesis = find_s(data)\n",
    "print(\"Final Hypothesis:\", hypothesis)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# data = pd.read_csv(\"Finds.csv\")\n",
    "# attribute = np.array(data)[:, :-1]\n",
    "# target = np.array(data)[:, -1]\n",
    "# print(attribute)\n",
    "# print(target)\n",
    "\n",
    "# def train(att, tar):\n",
    "#     specific_h = None\n",
    "\n",
    "#     for i, val in enumerate(tar):\n",
    "#         if val == 'yes':\n",
    "#             specific_h = att[i].copy()\n",
    "#             print(f\"Initial specific hypothesis (first 'yes' instance): {specific_h}\")\n",
    "#             break\n",
    "\n",
    "#     for i, val in enumerate(att):\n",
    "#         if tar[i] == 'yes':\n",
    "#             print(f\"\\nConsidering instance {i + 1}: {val}\")\n",
    "#             for x in range(len(specific_h)):\n",
    "#                 if val[x] != specific_h[x]:\n",
    "#                     specific_h[x] = '?'\n",
    "#             print(f\"Updated specific hypothesis: {specific_h}\")\n",
    "\n",
    "#     return specific_h\n",
    "\n",
    "# specific_hypothesis = train(attribute, target)\n",
    "# print(\"\\nFinal specific hypothesis:\", specific_hypothesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Candidate Elimination Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', '?', '?', '?', 0, 0]\n",
      "[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# data = pd.read_csv(r'E:\\ML LAB\\1ds.csv')\n",
    "\n",
    "# concept = np.array(data)[:,:-1]\n",
    "# target = np.array(data)[:,-1]\n",
    "\n",
    "# def train(con, tar):\n",
    "#     specific_h = con[0].copy()\n",
    "#     general_h = [['?' for _ in range(len(specific_h))] for _ in range(len(specific_h))]\n",
    "    \n",
    "#     for i, val in enumerate(con):\n",
    "#         if tar[i] == 'yes':\n",
    "#             for x in range(len(specific_h)):\n",
    "#                 if val[x]!= specific_h[x]:\n",
    "#                     specific_h[x] = '?'\n",
    "#                     general_h[x][x] = '?'\n",
    "#         else:\n",
    "#             for x in range(len(specific_h)):\n",
    "#                 if val[x]!= specific_h[x]:\n",
    "#                     general_h[x][x] = specific_h[x]\n",
    "#                 else:\n",
    "#                     general_h[x][x] = '?'\n",
    "        \n",
    "#         print(\"Iteration[\"+ str(i+1) +\"]\")\n",
    "#         print(\"Specific: \"+str(specific_h))\n",
    "#         print(\"General: \"+str(general_h)+\"\\n\\n\")\n",
    "        \n",
    "#     general_h = [general_h[i] for i, val in enumerate(general_h) if val!= ['?' for _ in range(len(specific_h))]]\n",
    "#     return specific_h, general_h  \n",
    "\n",
    "# specific, general = train(concept, target)\n",
    "\n",
    "# print(\"Final hypothesis :-\")\n",
    "# print(\"Specific hypothesis:\" +str(specific))\n",
    "# print(\"General hypothesis:\" +str(general))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(r'E:\\ML LAB\\1ds.csv',sep=\",\",header=None)\n",
    "#Intialize S and G\n",
    "S=[0,0,0,0,0,0]\n",
    "G=list()\n",
    "for i in range(len(df.columns)-1):\n",
    "    G.append(['?','?','?','?','?','?'])\n",
    "#Read samples\n",
    "for i in range(len(df)):\n",
    "    for j in range(len(df.columns)-1):\n",
    "        if df.iloc[i,-1]==\"Yes\":\n",
    "            if S[j]==0:\n",
    "                S[j]=df.iloc[i,j]\n",
    "            elif df.iloc[i,j]!=S[j]:\n",
    "                S[j]=\"?\"\n",
    "            if G[j][j]!='?' and S[j]=='?':\n",
    "                G[j][j]='?'\n",
    "        else:\n",
    "            if df.iloc[i,j]!=S[j] and S[j]!='?':\n",
    "                G[j][j]=S[j]\n",
    "print(S)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "def read_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "def build_decision_tree(data):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    tree.fit(X, y)\n",
    "    return tree\n",
    "\n",
    "def print_tree(tree, feature_names):\n",
    "    tree_rules = export_text(tree, feature_names=feature_names)\n",
    "    print(tree_rules)\n",
    "\n",
    "# Example usage:\n",
    "data = read_csv('3ds.csv')\n",
    "tree = build_decision_tree(data)\n",
    "print_tree(tree, data.columns[:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Nueral Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Build the neural network model\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', #Loss function used for classification tasks\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load Data\n",
    "# Fetch the 20 newsgroups dataset\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the Na√Øve Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Bayesian Belief Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the structure of the Bayesian Network\n",
    "model = BayesianModel([('Fever', 'Disease'), ('Chills', 'Disease'), ('Headache', 'Disease')])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    [True, True, True],  # Fever, Chills, Headache\n",
    "    [True, True, False],\n",
    "    [True, False, True],\n",
    "    [False, True, False],\n",
    "    [False, False, False],\n",
    "]\n",
    "\n",
    "# Group the data by the first two symptoms\n",
    "grouped_data = defaultdict(list)\n",
    "for row in data:\n",
    "    grouped_data[(row[0], row[1])].append(row[2])\n",
    "\n",
    "# Convert the grouped data into a contingency table\n",
    "contingency_table = {}\n",
    "for symptoms, values in grouped_data.items():\n",
    "    contingency_table[(symptoms, True)] = sum(values)\n",
    "    contingency_table[(symptoms, False)] = len(values) - sum(values)\n",
    "\n",
    "# Define the conditional probability distributions (CPDs)\n",
    "cpd_fever = TabularCPD(variable='Fever', variable_card=2, values=[[0.6], [0.4]])\n",
    "cpd_chills = TabularCPD(variable='Chills', variable_card=2, values=[[0.7], [0.3]])\n",
    "cpd_headache = TabularCPD(variable='Headache', variable_card=2, values=[[0.8], [0.2]])\n",
    "\n",
    "# For Disease, define the CPD with all 8 combinations of Fever, Chills, and Headache\n",
    "cpd_disease = TabularCPD(\n",
    "    variable='Disease', \n",
    "    variable_card=2, \n",
    "    values=[\n",
    "        [0.9, 0.7, 0.8, 0.3, 0.4, 0.5, 0.1, 0.2],  # Probabilities for Disease=True\n",
    "        [0.1, 0.3, 0.2, 0.7, 0.6, 0.5, 0.9, 0.8]   # Probabilities for Disease=False\n",
    "    ],\n",
    "    evidence=['Fever', 'Chills', 'Headache'], \n",
    "    evidence_card=[2, 2, 2]\n",
    ")\n",
    "\n",
    "# Add the CPDs to the model\n",
    "model.add_cpds(cpd_fever, cpd_chills, cpd_headache, cpd_disease)\n",
    "\n",
    "# Verify the model\n",
    "model.check_model()\n",
    "\n",
    "# Query the network\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "inference = VariableElimination(model)\n",
    "result = inference.map_query(variables=['Disease'], evidence={'Fever': 1, 'Chills': 1, 'Headache': 1})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "df=pd.read_csv(\"Medical Dataset.csv\")\n",
    "df=df.replace(\"?\",np.nan)\n",
    "model=BayesianModel([('age','heartdisease'),('sex','heartdisease'),\n",
    "('exang','heartdisease'),('cp','heartdisease'),\n",
    "('heartdisease','restecg'),('heartdisease','chol')])\n",
    "model.fit(df,estimator=MaximumLikelihoodEstimator)\n",
    "infer=VariableElimination(model)\n",
    "q=infer.query(variables=['heartdisease'],evidence={'restecg':1})\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Clustering Using K-means \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "df=pd.read_csv('iris.csv')\n",
    "x=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "colormap=np.array([\"red\",\"green\",\"blue\"])\n",
    "y=y.astype('category')\n",
    "y=y.cat.codes\n",
    "gm=GaussianMixture(n_components=3)\n",
    "gm.fit(x)\n",
    "gmc=gm.predict(x)\n",
    "km=KMeans(n_clusters=3)\n",
    "km.fit(x)\n",
    "kmc=km.predict(x)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x.iloc[:,0],x.iloc[:,1],c=colormap[y],s=40)\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x.iloc[:,0],x.iloc[:,1],c=colormap[gmc],s=40)\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x.iloc[:,0],x.iloc[:,1],c=colormap[kmc],s=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "n_samples = 100\n",
    "n_features = 3\n",
    "X = np.random.rand(n_samples, n_features) * 10  # Scale the data for better visualization\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3'])\n",
    "df.to_csv('random_points.csv', index=False)\n",
    "x = df.iloc[:, :-1]\n",
    "y = np.random.randint(0, 3, n_samples)\n",
    "\n",
    "# Define colormap\n",
    "colormap = np.array([\"red\", \"green\", \"blue\"])\n",
    "\n",
    "# Gaussian Mixture Model\n",
    "gm = GaussianMixture(n_components=3)\n",
    "gm.fit(x)\n",
    "gmc = gm.predict(x)\n",
    "\n",
    "# KMeans Clustering\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(x)\n",
    "kmc = km.predict(x)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(x.iloc[:, 0], x.iloc[:, 1], c=colormap[y], s=40)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(x.iloc[:, 0], x.iloc[:, 1], c=colormap[gmc], s=40)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(x.iloc[:, 0], x.iloc[:, 1], c=colormap[kmc], s=40)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def read_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "def knn_classifier(data, k):\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    \n",
    "    # Data normalization\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "    return accuracy, conf_matrix, class_report\n",
    "data = read_csv('8ds.csv')\n",
    "accuracy, conf_matrix, class_report = knn_classifier(data, 3)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def locally_weighted_regression(X, y, tau):\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        w = np.exp(-np.sum((X - X[i, :])**2, axis=1) / (2 * tau**2))\n",
    "        W = np.diag(w)\n",
    "        theta = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)\n",
    "        y_pred[i] = X[i, :] @ theta\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[1, x] for x in np.linspace(1, 10, 10)])\n",
    "y = np.array([1.0, 2.2, 2.8, 4.4, 5.0, 6.1, 7.2, 7.8, 9.1, 10.2])\n",
    "tau = 0.5\n",
    "y_pred = locally_weighted_regression(X, y, tau)\n",
    "\n",
    "plt.scatter(X[:, 1], y, color='blue', label='Data')\n",
    "plt.plot(X[:, 1], y_pred, color='red', label='Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def read_csv(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# def preprocess_data(data):\n",
    "#     # Check if data contains any non-numerical columns\n",
    "#     print(\"Data types:\\n\", data.dtypes)\n",
    "#     print(\"Data head:\\n\", data.head())\n",
    "    \n",
    "#     # Ensure no missing values\n",
    "#     print(\"Missing values:\\n\", data.isnull().sum())\n",
    "    \n",
    "#     return data\n",
    "\n",
    "def svm_classifier(data):\n",
    "    X = data.iloc[:, :-1]  # Features\n",
    "    y = data.iloc[:, -1]   # Target\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = SVC()  # Support Vector Classification\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "    \n",
    "    return accuracy, conf_matrix, class_report\n",
    "\n",
    "# Example usage:\n",
    "data = read_csv('10ds.csv')\n",
    "# data = preprocess_data(data)  # Ensure data is properly prepared\n",
    "accuracy, conf_matrix, class_report = svm_classifier(data)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
